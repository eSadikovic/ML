{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi everyon if you can read this messag your proper use parseouttext pleas proceed to the next part of the project\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import string\n",
    "stemmer=SnowballStemmer(\"english\")\n",
    "\n",
    "def parseOutText(f):\n",
    "    \"\"\" given an opened email file f, parse out all text below the\n",
    "        metadata block at the top\n",
    "        (in Part 2, you will also add stemming capabilities)\n",
    "        and return a string that contains all the words\n",
    "        in the email (space-separated) \n",
    "        \n",
    "        example use case:\n",
    "        f = open(\"email_file_name.txt\", \"r\")\n",
    "        text = parseOutText(f)\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "    f.seek(0)  ### go back to beginning of file (annoying)\n",
    "    all_text = f.read()\n",
    "\n",
    "    ### split off metadata\n",
    "    content = all_text.split(\"X-FileName:\")\n",
    "    words = \"\"\n",
    "    if len(content) > 1:\n",
    "        ### remove punctuation\n",
    "        text_string = content[1].translate(string.maketrans(\"\", \"\"), string.punctuation)\n",
    "\n",
    "        ### project part 2: comment out the line below\n",
    "        words = text_string\n",
    "\n",
    "        ### split the text string into individual words, stem each word,\n",
    "        ### and append the stemmed word to words (make sure there's a single\n",
    "        ### space between each stemmed word)\n",
    "        \n",
    "        l=text_string.split()\n",
    "        \n",
    "        for i in range(len(l)):\n",
    "            l[i]=stemmer.stem(l[i])\n",
    "            \n",
    "        word_list=[]\n",
    "        for j in l:\n",
    "            word_list.append(j)\n",
    "            \n",
    "        words = ' '.join(word_list)    \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return words\n",
    "\n",
    "    \n",
    "\n",
    "def main():\n",
    "    ff = open(\"../text_learning/test_email.txt\", \"r\")\n",
    "    text = parseOutText(ff)\n",
    "    print text\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Hi Everyone  If you can read this message youre properly using parseOutText  Please proceed to the next part of the project\n",
      "\n",
      "..\\maildir/bailey-s/deleted_items/101.\n",
      "..\\maildir/bailey-s/deleted_items/106.\n",
      "..\\maildir/bailey-s/deleted_items/132.\n",
      "..\\maildir/bailey-s/deleted_items/185.\n",
      "..\\maildir/bailey-s/deleted_items/186.\n",
      "..\\maildir/bailey-s/deleted_items/187.\n",
      "..\\maildir/bailey-s/deleted_items/193.\n",
      "..\\maildir/bailey-s/deleted_items/195.\n",
      "..\\maildir/bailey-s/deleted_items/214.\n",
      "..\\maildir/bailey-s/deleted_items/215.\n",
      "..\\maildir/bailey-s/deleted_items/233.\n",
      "..\\maildir/bailey-s/deleted_items/242.\n",
      "..\\maildir/bailey-s/deleted_items/243.\n",
      "..\\maildir/bailey-s/deleted_items/244.\n",
      "..\\maildir/bailey-s/deleted_items/246.\n",
      "..\\maildir/bailey-s/deleted_items/247.\n",
      "..\\maildir/bailey-s/deleted_items/254.\n",
      "..\\maildir/bailey-s/deleted_items/259.\n",
      "..\\maildir/bailey-s/deleted_items/260.\n",
      "..\\maildir/bailey-s/deleted_items/261.\n",
      "..\\maildir/bailey-s/deleted_items/263.\n",
      "..\\maildir/bailey-s/deleted_items/278.\n",
      "..\\maildir/bailey-s/deleted_items/290.\n",
      "..\\maildir/bailey-s/deleted_items/296.\n",
      "..\\maildir/bailey-s/deleted_items/302.\n",
      "..\\maildir/bailey-s/deleted_items/306.\n",
      "..\\maildir/bailey-s/deleted_items/307.\n",
      "..\\maildir/bailey-s/deleted_items/317.\n",
      "..\\maildir/bailey-s/deleted_items/320.\n",
      "..\\maildir/bailey-s/deleted_items/335.\n",
      "..\\maildir/bailey-s/deleted_items/70.\n",
      "..\\maildir/heard-m/brokerage_agreements/1.\n",
      "..\\maildir/heard-m/brokerage_agreements/16.\n",
      "..\\maildir/heard-m/brokerage_agreements/17.\n",
      "..\\maildir/heard-m/brokerage_agreements/2.\n",
      "..\\maildir/heard-m/brokerage_agreements/24.\n",
      "..\\maildir/heard-m/brokerage_agreements/26.\n",
      "..\\maildir/heard-m/brokerage_agreements/27.\n",
      "..\\maildir/heard-m/brokerage_agreements/33.\n",
      "..\\maildir/heard-m/brokerage_agreements/35.\n",
      "..\\maildir/heard-m/brokerage_agreements/36.\n",
      "..\\maildir/heard-m/brokerage_agreements/4.\n",
      "..\\maildir/heard-m/brokerage_agreements/52.\n",
      "..\\maildir/heard-m/brokerage_agreements/73.\n",
      "..\\maildir/heard-m/brokerage_agreements/8.\n",
      "..\\maildir/heard-m/brokerage_agreements/87.\n",
      "..\\maildir/heard-m/deleted_items/122.\n",
      "..\\maildir/heard-m/deleted_items/134.\n",
      "..\\maildir/heard-m/deleted_items/38.\n",
      "..\\maildir/heard-m/deleted_items/48.\n",
      "..\\maildir/heard-m/deleted_items/49.\n",
      "..\\maildir/heard-m/deleted_items/50.\n",
      "..\\maildir/heard-m/deleted_items/60.\n",
      "..\\maildir/heard-m/deleted_items/66.\n",
      "..\\maildir/heard-m/deleted_items/71.\n",
      "..\\maildir/heard-m/deleted_items/79.\n",
      "..\\maildir/heard-m/inbox/102.\n",
      "..\\maildir/heard-m/inbox/104.\n",
      "..\\maildir/heard-m/inbox/105.\n",
      "..\\maildir/heard-m/inbox/106.\n",
      "..\\maildir/heard-m/inbox/112.\n",
      "..\\maildir/heard-m/inbox/133.\n",
      "..\\maildir/heard-m/inbox/160.\n",
      "..\\maildir/heard-m/inbox/161.\n",
      "..\\maildir/heard-m/inbox/166.\n",
      "..\\maildir/heard-m/inbox/177.\n",
      "..\\maildir/heard-m/inbox/182.\n",
      "..\\maildir/heard-m/inbox/19.\n",
      "..\\maildir/heard-m/inbox/21.\n",
      "..\\maildir/heard-m/inbox/22.\n",
      "..\\maildir/heard-m/inbox/23.\n",
      "..\\maildir/heard-m/inbox/26.\n",
      "..\\maildir/heard-m/inbox/275.\n",
      "..\\maildir/heard-m/inbox/34.\n",
      "..\\maildir/heard-m/inbox/35.\n",
      "..\\maildir/heard-m/inbox/41.\n",
      "..\\maildir/heard-m/inbox/48.\n",
      "..\\maildir/heard-m/inbox/67.\n",
      "..\\maildir/heard-m/inbox/69.\n",
      "..\\maildir/heard-m/inbox/76.\n",
      "..\\maildir/hodge-j/deleted_items/206.\n",
      "..\\maildir/hodge-j/inbox/436.\n",
      "..\\maildir/jones-t/all_documents/10126.\n",
      "..\\maildir/jones-t/all_documents/10138.\n",
      "..\\maildir/jones-t/all_documents/10347.\n",
      "..\\maildir/jones-t/all_documents/10602.\n",
      "..\\maildir/jones-t/all_documents/10865.\n",
      "..\\maildir/jones-t/all_documents/10946.\n",
      "..\\maildir/jones-t/all_documents/11004.\n",
      "..\\maildir/jones-t/all_documents/11404.\n",
      "..\\maildir/jones-t/all_documents/11696.\n",
      "..\\maildir/jones-t/all_documents/11799.\n",
      "..\\maildir/jones-t/all_documents/12130.\n",
      "..\\maildir/jones-t/all_documents/12231.\n",
      "..\\maildir/jones-t/all_documents/1238.\n",
      "..\\maildir/jones-t/all_documents/1334.\n",
      "..\\maildir/jones-t/all_documents/1373.\n",
      "..\\maildir/jones-t/all_documents/1397.\n",
      "..\\maildir/jones-t/all_documents/1465.\n",
      "..\\maildir/jones-t/all_documents/1469.\n",
      "..\\maildir/jones-t/all_documents/1534.\n",
      "..\\maildir/jones-t/all_documents/1566.\n",
      "..\\maildir/jones-t/all_documents/1607.\n",
      "..\\maildir/jones-t/all_documents/1608.\n",
      "..\\maildir/jones-t/all_documents/1661.\n",
      "..\\maildir/jones-t/all_documents/1693.\n",
      "..\\maildir/jones-t/all_documents/1727.\n",
      "..\\maildir/jones-t/all_documents/1746.\n",
      "..\\maildir/jones-t/all_documents/1760.\n",
      "..\\maildir/jones-t/all_documents/1848.\n",
      "..\\maildir/jones-t/all_documents/1936.\n",
      "..\\maildir/jones-t/all_documents/1970.\n",
      "..\\maildir/jones-t/all_documents/1976.\n",
      "..\\maildir/jones-t/all_documents/2035.\n",
      "..\\maildir/jones-t/all_documents/2091.\n",
      "..\\maildir/jones-t/all_documents/2117.\n",
      "..\\maildir/jones-t/all_documents/2172.\n",
      "..\\maildir/jones-t/all_documents/2189.\n",
      "..\\maildir/jones-t/all_documents/2205.\n",
      "..\\maildir/jones-t/all_documents/2313.\n",
      "..\\maildir/jones-t/all_documents/2316.\n",
      "..\\maildir/jones-t/all_documents/2344.\n",
      "..\\maildir/jones-t/all_documents/2448.\n",
      "..\\maildir/jones-t/all_documents/2759.\n",
      "..\\maildir/jones-t/all_documents/2798.\n",
      "..\\maildir/jones-t/all_documents/2827.\n",
      "..\\maildir/jones-t/all_documents/2847.\n",
      "..\\maildir/jones-t/all_documents/2859.\n",
      "..\\maildir/jones-t/all_documents/2910.\n",
      "..\\maildir/jones-t/all_documents/2983.\n",
      "..\\maildir/jones-t/all_documents/3028.\n",
      "..\\maildir/jones-t/all_documents/3058.\n",
      "..\\maildir/jones-t/all_documents/3081.\n",
      "..\\maildir/jones-t/all_documents/3101.\n",
      "..\\maildir/jones-t/all_documents/3321.\n",
      "..\\maildir/jones-t/all_documents/3333.\n",
      "..\\maildir/jones-t/all_documents/3362.\n",
      "..\\maildir/jones-t/all_documents/3642.\n",
      "..\\maildir/jones-t/all_documents/3794.\n",
      "..\\maildir/jones-t/all_documents/3816.\n",
      "..\\maildir/jones-t/all_documents/3829.\n",
      "..\\maildir/jones-t/all_documents/3840.\n",
      "..\\maildir/jones-t/all_documents/3841.\n",
      "..\\maildir/jones-t/all_documents/3857.\n",
      "..\\maildir/jones-t/all_documents/3858.\n",
      "..\\maildir/jones-t/all_documents/3873.\n",
      "..\\maildir/jones-t/all_documents/3921.\n",
      "..\\maildir/jones-t/all_documents/3961.\n",
      "..\\maildir/jones-t/all_documents/3973.\n",
      "..\\maildir/jones-t/all_documents/3993.\n",
      "..\\maildir/jones-t/all_documents/3997.\n",
      "..\\maildir/jones-t/all_documents/4046.\n",
      "..\\maildir/jones-t/all_documents/4056.\n",
      "..\\maildir/jones-t/all_documents/4136.\n",
      "..\\maildir/jones-t/all_documents/4161.\n",
      "..\\maildir/jones-t/all_documents/4212.\n",
      "..\\maildir/jones-t/all_documents/4244.\n",
      "..\\maildir/jones-t/all_documents/4249.\n",
      "..\\maildir/jones-t/all_documents/4251.\n",
      "..\\maildir/jones-t/all_documents/4255.\n",
      "..\\maildir/jones-t/all_documents/4496.\n",
      "..\\maildir/jones-t/all_documents/4650.\n",
      "..\\maildir/jones-t/all_documents/4654.\n",
      "..\\maildir/jones-t/all_documents/4710.\n",
      "..\\maildir/jones-t/all_documents/4741.\n",
      "..\\maildir/jones-t/all_documents/4743.\n",
      "..\\maildir/jones-t/all_documents/4757.\n",
      "..\\maildir/jones-t/all_documents/4819.\n",
      "..\\maildir/jones-t/all_documents/4895.\n",
      "..\\maildir/jones-t/all_documents/4938.\n",
      "..\\maildir/jones-t/all_documents/4968.\n",
      "..\\maildir/jones-t/all_documents/5035.\n",
      "..\\maildir/jones-t/all_documents/5055.\n",
      "..\\maildir/jones-t/all_documents/5063.\n",
      "..\\maildir/jones-t/all_documents/5419.\n",
      "..\\maildir/jones-t/all_documents/5519.\n",
      "..\\maildir/jones-t/all_documents/5523.\n",
      "..\\maildir/jones-t/all_documents/5555.\n",
      "..\\maildir/jones-t/all_documents/5575.\n",
      "..\\maildir/jones-t/all_documents/5588.\n",
      "..\\maildir/jones-t/all_documents/5589.\n",
      "..\\maildir/jones-t/all_documents/5646.\n",
      "..\\maildir/jones-t/all_documents/5654.\n",
      "..\\maildir/jones-t/all_documents/5691.\n",
      "..\\maildir/jones-t/all_documents/5693.\n",
      "..\\maildir/jones-t/all_documents/5751.\n",
      "..\\maildir/jones-t/all_documents/5859.\n",
      "..\\maildir/jones-t/all_documents/5884.\n",
      "..\\maildir/jones-t/all_documents/5928.\n",
      "..\\maildir/jones-t/all_documents/5936.\n",
      "..\\maildir/jones-t/all_documents/5948.\n",
      "..\\maildir/jones-t/all_documents/9401.\n",
      "..\\maildir/jones-t/all_documents/9408.\n",
      "..\\maildir/jones-t/all_documents/9430.\n",
      "..\\maildir/jones-t/all_documents/9550.\n",
      "..\\maildir/jones-t/all_documents/9627.\n",
      "..\\maildir/jones-t/all_documents/9808.\n",
      "..\\maildir/jones-t/deleted_items/45.\n",
      "..\\maildir/jones-t/inbox/742.\n",
      "Entry #152:   MHEARD NonPrivilegedpst\n",
      "\n",
      "Sheila  please see my comments below about the correct Enron company for only futures and options appropriate type of transaction\n",
      "\n",
      "\n",
      "\n",
      " Original Message\n",
      "From \tGlover Sheila  \n",
      "Sent\tMonday October 22 2001 912 AM\n",
      "To\tShackleton Sara Heard Marie\n",
      "Subject\tFW Two New Executing Brokers\n",
      "\n",
      "\n",
      "Sara and Marie\n",
      "This was the contact for Monument\n",
      "Thanks Sheila\n",
      " Original Message\n",
      "From \tGlover Sheila  \n",
      "Sent\tMonday October 01 2001 1011 AM\n",
      "To\tShackleton Sara Heard Marie\n",
      "Subject\tFW Two New Executing Brokers\n",
      "\n",
      "SS and MH\n",
      "See addition of Deutche below\n",
      "thankssg\n",
      "\n",
      " Original Message\n",
      "From \tGlover Sheila  \n",
      "Sent\tMonday October 01 2001 913 AM\n",
      "To\tShackleton Sara Heard Marie\n",
      "Subject\tFW Two New Executing Brokers\n",
      "\n",
      "Sara and Marie\n",
      "How is Monument coming along  They use Credit Lyonnais as their back office\n",
      "We Shackleton Sara   ENA andor ECT Investments  will use MSDW and GSI for Futures and Options Shackleton Sara   or equityoptions deals \n",
      "We Shackleton Sara   ENA  will use Carr  Glover Sheila  and Deutche  for Futures\n",
      "\n",
      "I had given you a contact of Jackie Tokley 2073385740\n",
      "Thanks \n",
      "Sheila\n",
      "\n",
      " Original Message\n",
      "From \tGlover Sheila  \n",
      "Sent\tWednesday September 05 2001 405 PM\n",
      "To\tLowry Donna Schultz Cassandra Shackleton Sara Bradford William S\n",
      "Cc\tHeard Marie Hickerson Gary Towarek Michael\n",
      "Subject\tFW Two New Executing Brokers\n",
      "\n",
      "We are expanding our trading in the European Market  \n",
      "\n",
      "We will be forwarding to legal the documentation related to these two new Executing Brokers which will be utilized for order flow and research\n",
      "\n",
      "I will send to legal the required documentation and legal contacts\n",
      "\n",
      "Sheila\n",
      "\n",
      " Original Message\n",
      "From \tHopley Kimberly  \n",
      "Sent\tWednesday September 05 2001 727 AM\n",
      "To\tGlover Sheila\n",
      "Cc\tBrogan Theresa T Charania Aneela\n",
      "Subject\tTwo New Executing Brokers\n",
      "\n",
      "Sheila\n",
      "\n",
      "Mike would like the following two Institutions set up as executing brokers for our foreign business \n",
      "\n",
      "Monument Derivatives Ltd \n",
      "11 Old Jewry\n",
      "London\n",
      "EC2R 8DU\n",
      "Contacts  Andy Ash or Robert Green 0207 338 0825\n",
      "\n",
      "Metzler \n",
      "Grosse Gallustrasse 18\n",
      "60311 FrankfurtMain\n",
      "Germany\n",
      "Contacts  Daniela Tresin 0039 02 777 1552\n",
      "\n",
      "I have a copy of Monuments Terms of Business Agreement which asks for a signature do you want me to forward this document to you\n",
      "\n",
      "Please let me know If you would like me to do anything from this side\n",
      "\n",
      "Many thanks\n",
      "Kimberly Hopley\n",
      "Financial Trading\n",
      "Tel  44020 7783 2644\n",
      "Fax  44020 7783 1913\n",
      "Email  kimberlyhopleyenroncom \n",
      "emails processed\n",
      "3658\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "import pickle\n",
    "\n",
    "import re\n",
    "\n",
    "import sys\n",
    "\n",
    "\n",
    "\n",
    "sys.path.append( \"../tools/\" )\n",
    "\n",
    "from parse_out_email_text import parseOutText\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    starter code to process the emails from Sara and Chris to extract\n",
    "\n",
    "    the features and get the documents ready for classification\n",
    "\n",
    "\n",
    "\n",
    "    the list of all the emails from Sara are in the from_sara list\n",
    "\n",
    "    likewise for emails from Chris (from_chris)\n",
    "\n",
    "\n",
    "\n",
    "    the actual documents are in the Enron email dataset, which\n",
    "\n",
    "    you downloaded/unpacked in Part 0 of the first mini-project\n",
    "\n",
    "\n",
    "\n",
    "    the data is stored in lists and packed away in pickle files at the end\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "### Parse out test email\n",
    "\n",
    "testEmail = open(\"test_email.txt\", \"r\")\n",
    "\n",
    "print parseOutText(testEmail)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from_sara  = open(\"from_sara.txt\", \"r\")\n",
    "\n",
    "from_chris = open(\"from_chris.txt\", \"r\")\n",
    "\n",
    "\n",
    "\n",
    "from_data = []\n",
    "\n",
    "word_data = []\n",
    "\n",
    "\n",
    "\n",
    "### temp_counter is a way to speed up the development--there are\n",
    "\n",
    "### thousands of emails from Sara and Chris, so running over all of them\n",
    "\n",
    "### can take a long time\n",
    "\n",
    "### temp_counter helps you only look at the first 200 emails in the list\n",
    "\n",
    "temp_counter = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for name, from_person in [(\"sara\", from_sara), (\"chris\", from_chris)]:\n",
    "\n",
    "    for path in from_person:\n",
    "\n",
    "        ### only look at first 200 emails when developing\n",
    "\n",
    "        ### once everything is working, remove this line to run over full dataset\n",
    "\n",
    "        temp_counter += 1\n",
    "\n",
    "        if temp_counter < 200:\n",
    "\n",
    "            path = os.path.join('..', path[:-1])\n",
    "\n",
    "            print path\n",
    "\n",
    "            email = open(path, \"r\")\n",
    "\n",
    "\n",
    "\n",
    "            ### use parseOutText to extract the text from the opened email\n",
    "\n",
    "            ### use str.replace() to remove any instances of the words\n",
    "\n",
    "            ### [\"sara\", \"shackleton\", \"chris\", \"germani\"]\n",
    "\n",
    "            words = parseOutText(email)\n",
    "\n",
    "            remove  = [\"sara\", \"shackleton\", \"chris\", \"germani\"]\n",
    "\n",
    "            #list_rep  = [\"sara\", \"shackleton\", \"chris\", \"germani\",\"sshacklensf\",\"cgermannsf\"]\n",
    "\n",
    "            for word in remove:\n",
    "\n",
    "                words = words.replace(word,\"\")\n",
    "\n",
    "            ### append the text to word_data\n",
    "\n",
    "            word_data.append(words)\n",
    "\n",
    "            ### append a 0 to from_data if email is from Sara, and 1 if email is from Chris\n",
    "\n",
    "            if name == \"sara\":\n",
    "                from_data.append(0)\n",
    "            elif name == \"chris\":\n",
    "                from_data.append(1)\n",
    "        \n",
    "        ### use parseOutText to extract the text from the opened email\n",
    "            text = parseOutText(email)\n",
    "        \n",
    "        ### use str.replace() to remove any instances of the words\n",
    "        ### [\"sara\", \"shackleton\", \"chris\", \"germani\"]\n",
    "            for i in [\"sara\",\"shackleton\",\"chris\",\"germani\",\"sshacklensf\",\"cgermannsf\"]:\n",
    "                text = text.replace(i,\"\")\n",
    "                                \n",
    "        ### append the text to word_data\n",
    "            word_data.append(text)\n",
    "            email.close()\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "            email.close()\n",
    "\n",
    "\n",
    "\n",
    "print \"Entry #152: \", word_data[152]\n",
    "\n",
    "\n",
    "\n",
    "print \"emails processed\"\n",
    "\n",
    "from_sara.close()\n",
    "\n",
    "from_chris.close()\n",
    "\n",
    "\n",
    "\n",
    "pickle.dump( word_data, open(\"your_word_data.pkl\", \"w\") )\n",
    "\n",
    "pickle.dump( from_data, open(\"your_email_authors.pkl\", \"w\") )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### in Part 4, do TfIdf vectorization here\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "sw = stopwords.words(\"english\")\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\",lowercase=True)\n",
    "\n",
    "vectorizer.fit_transform(word_data)\n",
    "\n",
    "# bag_words = vectorizer.transform(word_data)\n",
    "\n",
    "\n",
    "\n",
    "# Get how many unique words there are in the emails\n",
    "\n",
    "print len(vectorizer.get_feature_names())\n",
    "\n",
    "\n",
    "\n",
    "# Word number 34597\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print word_data[152]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' tjonesnsf\\n\\nStephanie and Sam need NYMEX calendars'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_data[152]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'i'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sw[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i\n",
      "me\n",
      "my\n",
      "myself\n",
      "we\n",
      "our\n",
      "ours\n",
      "ourselves\n",
      "you\n",
      "your\n",
      "yours\n",
      "yourself\n",
      "yourselves\n",
      "he\n",
      "him\n",
      "his\n",
      "himself\n",
      "she\n",
      "her\n",
      "hers\n",
      "herself\n",
      "it\n",
      "its\n",
      "itself\n",
      "they\n",
      "them\n",
      "their\n",
      "theirs\n",
      "themselves\n",
      "what\n",
      "which\n",
      "who\n",
      "whom\n",
      "this\n",
      "that\n",
      "these\n",
      "those\n",
      "am\n",
      "is\n",
      "are\n",
      "was\n",
      "were\n",
      "be\n",
      "been\n",
      "being\n",
      "have\n",
      "has\n",
      "had\n",
      "having\n",
      "do\n",
      "does\n",
      "did\n",
      "doing\n",
      "a\n",
      "an\n",
      "the\n",
      "and\n",
      "but\n",
      "if\n",
      "or\n",
      "because\n",
      "as\n",
      "until\n",
      "while\n",
      "of\n",
      "at\n",
      "by\n",
      "for\n",
      "with\n",
      "about\n",
      "against\n",
      "between\n",
      "into\n",
      "through\n",
      "during\n",
      "before\n",
      "after\n",
      "above\n",
      "below\n",
      "to\n",
      "from\n",
      "up\n",
      "down\n",
      "in\n",
      "out\n",
      "on\n",
      "off\n",
      "over\n",
      "under\n",
      "again\n",
      "further\n",
      "then\n",
      "once\n",
      "here\n",
      "there\n",
      "when\n",
      "where\n",
      "why\n",
      "how\n",
      "all\n",
      "any\n",
      "both\n",
      "each\n",
      "few\n",
      "more\n",
      "most\n",
      "other\n",
      "some\n",
      "such\n",
      "no\n",
      "nor\n",
      "not\n",
      "only\n",
      "own\n",
      "same\n",
      "so\n",
      "than\n",
      "too\n",
      "very\n",
      "s\n",
      "t\n",
      "can\n",
      "will\n",
      "just\n",
      "don\n",
      "should\n",
      "now\n",
      "d\n",
      "ll\n",
      "m\n",
      "o\n",
      "re\n",
      "ve\n",
      "y\n",
      "ain\n",
      "aren\n",
      "couldn\n",
      "didn\n",
      "doesn\n",
      "hadn\n",
      "hasn\n",
      "haven\n",
      "isn\n",
      "ma\n",
      "mightn\n",
      "mustn\n",
      "needn\n",
      "shan\n",
      "shouldn\n",
      "wasn\n",
      "weren\n",
      "won\n",
      "wouldn\n"
     ]
    }
   ],
   "source": [
    "for i in sw:\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'responsiv'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer=SnowballStemmer(\"english\")\n",
    "stemmer.stem('responsivness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'responsiv'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('responsiv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'respond'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('respond')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'respons'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('responsivity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'unrespons'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('unresponsive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 7, 8]\n"
     ]
    }
   ],
   "source": [
    "l=[1,2,3]\n",
    "\n",
    "for i in range(len(l)):\n",
    "    l[i]=l[i]+5\n",
    "    \n",
    "print l    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'list'>\n",
      "<type 'list'>\n",
      "<type 'NoneType'>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-edfee43e19c2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwordl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mwordl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwordl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "l=[1,2,3,4,5,6]\n",
    "wordl=[]\n",
    "print type(wordl)\n",
    "for j in l:\n",
    "    print type(wordl)\n",
    "    wordl=wordl.append(j)\n",
    "            \n",
    "words = ' '.join(word_list)  \n",
    "print wordl\n",
    "print words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi everyon if you can read this messag your proper use parseouttext pleas proceed to the next part of the project\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import string\n",
    "\n",
    "def parseOutText(f):\n",
    "    \"\"\" given an opened email file f, parse out all text below the\n",
    "        metadata block at the top\n",
    "        (in Part 2, you will also add stemming capabilities)\n",
    "        and return a string that contains all the words\n",
    "        in the email (space-separated) \n",
    "        \n",
    "        example use case:\n",
    "        f = open(\"email_file_name.txt\", \"r\")\n",
    "        text = parseOutText(f)\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "    f.seek(0)  ### go back to beginning of file (annoying)\n",
    "    all_text = f.read()\n",
    "\n",
    "    ### split off metadata\n",
    "    content = all_text.split(\"X-FileName:\")\n",
    "    words = \"\"\n",
    "    if len(content) > 1:\n",
    "        ### remove punctuation\n",
    "        text_string = content[1].translate(string.maketrans(\"\", \"\"), string.punctuation)\n",
    "\n",
    "        ### project part 2: comment out the line below\n",
    "        words = text_string\n",
    "\n",
    "        ### split the text string into individual words, stem each word,\n",
    "        ### and append the stemmed word to words (make sure there's a single\n",
    "        ### space between each stemmed word)\n",
    "        \n",
    "        l=text_string.split()\n",
    "        \n",
    "        for i in range(len(l)):\n",
    "            l[i]=stemmer.stem(l[i])\n",
    "            \n",
    "        word_list=[]\n",
    "        for j in l:\n",
    "            word_list.append(j)\n",
    "            \n",
    "        words = ' '.join(word_list)    \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return words\n",
    "\n",
    "    \n",
    "\n",
    "def main():\n",
    "    ff = open(\"../text_learning/test_email.txt\", \"r\")\n",
    "    text = parseOutText(ff)\n",
    "    print text\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import string\n",
    "\n",
    "def parseOutText(f):\n",
    "    \"\"\" given an opened email file f, parse out all text below the\n",
    "        metadata block at the top\n",
    "        (in Part 2, you will also add stemming capabilities)\n",
    "        and return a string that contains all the words\n",
    "        in the email (space-separated) \n",
    "        \n",
    "        example use case:\n",
    "        f = open(\"email_file_name.txt\", \"r\")\n",
    "        text = parseOutText(f)\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "    f.seek(0)  ### go back to beginning of file (annoying)\n",
    "    all_text = f.read()\n",
    "\n",
    "    ### split off metadata\n",
    "    content = all_text.split(\"X-FileName:\")\n",
    "    words = \"\"\n",
    "    if len(content) > 1:\n",
    "        ### remove punctuation\n",
    "        text_string = content[1].translate(string.maketrans(\"\", \"\"), string.punctuation)\n",
    "\n",
    "        ### project part 2: comment out the line below\n",
    "        words = text_string\n",
    "\n",
    "        ### split the text string into individual words, stem each word,\n",
    "        ### and append the stemmed word to words (make sure there's a single\n",
    "        ### space between each stemmed word)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return words\n",
    "\n",
    "    \n",
    "\n",
    "def main():\n",
    "    ff = open(\"../text_learning/test_email.txt\", \"r\")\n",
    "    text = parseOutText(ff)\n",
    "    print text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
